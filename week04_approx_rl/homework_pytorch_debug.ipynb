{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can get 1 pt (instead of 3 pts) for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioIEVODJZdC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503f0cb2-775c-411c-c4ed-ad3c9dd72970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    os.makedirs('dqn', exist_ok=True)\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/framebuffer.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/analysis.py -P dqn/\n",
        "\n",
        "    !pip install gymnasium\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import dqn.utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AmFXRrkqZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "5fd35237-b177-4788-eec0-f120f9ec0ba5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UVlpkvZOZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8674606-755d-49fe-ef2d-80e9b1d07de8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        assert len(state_shape) == 1\n",
        "        state_dim = state_shape[0]\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim,state_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(state_dim*4,state_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(state_dim*2,n_actions),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = self.network(state_t)\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert (\n",
        "            len(qvalues.shape) == 2 and\n",
        "            qvalues.shape[0] == state_t.shape[0] and\n",
        "            qvalues.shape[1] == n_actions\n",
        "        )\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bv1s5JKzZdC-"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e-Sg1cqPZdC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wQEHwR1AZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "18a4f789-3a5f-4ec5-a65c-5f91985fb1ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3627413312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experience replay size should be 10 because that's what maximum capacity is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dqn/replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[1;32m     70\u001b[0m         \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dqn/replay_buffer.py\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mobs_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mobses_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mobses_tp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to avoid copy while creating an array as requested.\nIf using `np.array(obj, copy=False)` replace it with `np.asarray(obj)` to allow a copy when needed (no behavior change in NumPy 1.x).\nFor more details, see https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword."
          ]
        }
      ],
      "source": [
        "from dqn.replay_buffer import ReplayBuffer\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0RnFX5sfZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a4e71a73-2b57-4f07-ef7e-f165b9284708"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-13-2932242177.py, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-13-2932242177.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    <YOUR CODE>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
        "    It is guaranteed that env has terminated=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXXmFEKGZdC_"
      },
      "outputs": [],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \\\n",
        "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
        "    \"but instead added %i\" % len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, \\\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
        "        np.mean(is_dones), len(exp_replay))\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \\\n",
        "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \\\n",
        "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (10,), \\\n",
        "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
        "        \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLJCNiuZdC_"
      },
      "outputs": [],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxrEOC7mZdC_"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \\\n",
        "    \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \\\n",
        "    \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \\\n",
        "    \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOk81bdZZdC_"
      },
      "outputs": [],
      "source": [
        "seed = <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13K5t2CTZdDA"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD7PAlwQZdDA"
      },
      "outputs": [],
      "source": [
        "from dqn.utils import is_enough_ram, linear_decay\n",
        "\n",
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not is_enough_ram(min_available_gb=0.1):\n",
        "        print(\"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "             )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        <YOUR CODE: sample batch_size of data from experience replay>\n",
        "\n",
        "        loss = <YOUR CODE: compute TD loss>\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm)\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            <YOUR CODE>\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history\")\n",
        "            plt.plot(td_loss_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history\")\n",
        "            plt.plot(grad_norm_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(\n",
        "  make_env(),\n",
        "  agent, n_games=30, greedy=True, t_max=1000\n",
        ")\n",
        "print('final score:', final_score)\n",
        "assert final_score > 300, 'not good enough for DQN'\n",
        "print('Well done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "from analysis import play_and_log_episode\n",
        "\n",
        "eval_env = make_env()\n",
        "record = play_and_log_episode(eval_env, agent)\n",
        "print('total reward for life:', np.sum(record['rewards']))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record['v_mc'], record['v_agent'])\n",
        "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
        "       'black', linestyle='--', label='x=y')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title('State Value Estimates')\n",
        "ax.set_xlabel('Monte-Carlo')\n",
        "ax.set_ylabel('Agent')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}